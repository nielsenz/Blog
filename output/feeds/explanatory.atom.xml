<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Zach Nielsen's Blog - Explanatory</title><link href="http://zachnielsen.org/" rel="alternate"></link><link href="http://zachnielsen.org/feeds/explanatory.atom.xml" rel="self"></link><id>http://zachnielsen.org/</id><updated>2017-01-15T00:00:00-05:00</updated><entry><title>AI Advancements</title><link href="http://zachnielsen.org/ai-advancements.html" rel="alternate"></link><published>2017-01-15T00:00:00-05:00</published><updated>2017-01-15T00:00:00-05:00</updated><author><name>Zach Nielsen</name></author><id>tag:zachnielsen.org,2017-01-15:/ai-advancements.html</id><summary type="html">&lt;p&gt;Keeping an eye on &lt;span class="caps"&gt;AI&lt;/span&gt; is definitely a New Years&amp;nbsp;resolution.&lt;/p&gt;</summary><content type="html">&lt;p&gt;While perusing &lt;a href="http://www.theverge.com/2017/1/10/14220578/ai-deepstack-beats-poker-pros-no-limit-texas-hold-em"&gt;The Verge&lt;/a&gt;, I came across an article about how researchers have created an &lt;span class="caps"&gt;AI&lt;/span&gt; that plays poker. This is a pretty incredible advancement, as poker is a much more complex game for computers to&amp;nbsp;learn. &lt;/p&gt;
&lt;p&gt;Poker is a game where all of the information is not known. Players hands are hidden from each other, which makes the algorithm more complex. This type of game differs to a game like chess, where the state of the board is known to both players of the game. Because there is no hidden information, the optimal move can be found much&amp;nbsp;quicker.  &lt;/p&gt;
&lt;p&gt;This is further evidence that the &lt;span class="caps"&gt;AI&lt;/span&gt; revolution is coming, and will take over in realms that were often thought to be exclusive to human. Reading &lt;a href="https://www.amazon.com/Second-Machine-Age-Prosperity-Technologies/dp/0393350649" title="Amazon link"&gt;The Second Machine Age&lt;/a&gt; made the case that &lt;span class="caps"&gt;AI&lt;/span&gt; will get better at a faster rate. The state of &lt;span class="caps"&gt;AI&lt;/span&gt; is worth keeping an eye&amp;nbsp;on. &lt;/p&gt;</content><category term="AI"></category></entry><entry><title>O(n) for data scientists</title><link href="http://zachnielsen.org/o-for-data-scientists.html" rel="alternate"></link><published>2016-09-20T00:00:00-04:00</published><updated>2016-09-20T00:00:00-04:00</updated><author><name>Zach Nielsen</name></author><id>tag:zachnielsen.org,2016-09-20:/o-for-data-scientists.html</id><summary type="html">&lt;p&gt;Coding just got less&amp;nbsp;complex.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Something that gets studied a lot in computer science is the idea of big O Notation. This basically signifies the complexity of the code that the computer is running. For example, if you give the computer some random data (call it  n random data), how many operations does the computer have to do to the data. This is measured as a function of&amp;nbsp;n.&lt;/p&gt;
&lt;p&gt;For example, if one wanted to sort data, the fastest sort that can be completed has a big O notation of O(nlog(n)). This says that the maximum number of times it touches the data is nlog(n), for any n amount of data. Therefore, a lot of time is spent trying to reduce the complexity for any algorithm, aka how many operations the computer has to do to each bit of code. The problem we&amp;#8217;re trying to solve is how can we make an algorithm go faster and implement fewer computations on the&amp;nbsp;data&lt;/p&gt;
&lt;h2&gt;Big O for data&amp;nbsp;scientists&lt;/h2&gt;
&lt;p&gt;This idea is something that data scientists can use in their own field, especially when writing &lt;span class="caps"&gt;SQL&lt;/span&gt; code. A lot of the code I run into is poorly written, and can be reordered in order to speed up the entire&amp;nbsp;process. &lt;/p&gt;
&lt;p&gt;For example, say I have 1 million entries that have to be standardized as well as rolled up, so that there are only 2000 groups. Now, one may think that standardizing and then rolling the data up is the best way to do that. That would lead to 1 million entries being standardized, and then that standardized data being rolled&amp;nbsp;up.&lt;/p&gt;
&lt;p&gt;However, if the process was reversed, as in the roll up occurs first and then the data standardized, that 1 million standardization operations that have to be performed gets brought down to 2000 (give or take). The query runs a lot faster, and you are able to be more productive while doing&amp;nbsp;so. &lt;/p&gt;
&lt;p&gt;In other words, the rollup has to be performed regardless. However, by standardizing the data after the rollup instead of before, fewer total entries need to be standardized, thus speeding up the entire&amp;nbsp;procedure. &lt;/p&gt;
&lt;h2&gt;Takeaways:&lt;/h2&gt;
&lt;p&gt;The main thing to remember is to think through the entire query, and then try to minimize the operations you do to the data. Can the rollup come first? Does the order matter? How can you reduce the stress on the machine? The difference between a good programmer and a great programmer is being able to write a faster procedure that solves the same&amp;nbsp;problem.&lt;/p&gt;</content><category term="data"></category><category term="complexity"></category><category term="sql"></category><category term="coding"></category></entry></feed>